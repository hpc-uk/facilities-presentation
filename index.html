<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>UK HPC Facilities</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">
		<link rel="stylesheet" href="css/local.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

<!-- Start of presentation ------------------------------------------------------------------------ -->
<section>
	<h1>HPC facilities for UK researchers</h1>
	<p>Andy Turner, EPCC<br/>
	<a href="mailto:a.turner@epcc.ed.ac.uk">a.turner@epcc.ed.ac.uk</a></p>
	<img height="100" class="plain" src="img/epcc_logo.png" alt="EPCC logo" />
	<img height="100" class="plain" src="img/hpcuk_logo.png" alt="HPC-UK logo" />
	<p style="font-size: 0.7em"><a href="https://hpc-uk.github.io/facilities-presentation/">https://hpc-uk.github.io/facilities-presentation/</a></p>
</section>

<section>

	<p style="font-size: 0.5em">Clone/fork this presentation at: <a href="https://github.com/hpc-uk/facilities-presentation">https://github.com/hpc-uk/facilities-presentation</a></p>
	<img class="plain" src="img/cc_licence.png" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License" />
	<p>Slide content is available under under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
        Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</a></p>

        <p style="font-size: 0.35em">This means you are free to copy and redistribute the material and adapt
		and build on the material under the following terms: You must give appropriate credit, provide
		a link to the license and indicate if changes were made. If you adapt or build on the material
		you must distribute your work under the same license as the original.<br/>
		Note that this presentation contains images owned by others. Please seek their permission
		before reusing these images.</p>

	<p>Built using <a href="https://github.com/hakimel/reveal.js">reveal.js</a></p>
	<p style="font-size: 0.35em">reveal.js is available under the
		<a href="https://github.com/hakimel/reveal.js/blob/master/LICENSE">MIT licence</a></p>

</section>

<section>
	<h2>Overview</h2>
	<ul>
		<li><a href="#/ukhpc">UK HPC Facilities</a>
			<ul class="inner">
				<li><a href="#/archer">ARCHER</a> - UK National Supercomputing Service</li>
				<li><a href="#/dirac">DiRAC</a> - STFC national HPC service</li>
				<li><a href="#/tier2">UK National Tier-2 HPC facilities</a></li>
				<li><a href="#/other">Other facilities</a></li>

			</ul></li>
		<li><a href="#/support">Getting Advice</a></li>
		<li><a href="#/summary">Summary</a></li>
	</ul>
</section>


<!-- HPC-UK Section ------------------------------------------------------------------------ -->

<section id="ukhpc">
	<h2>UK HPC Facilities</h2>
</section>

<section id="ukhpc">
	<h3>HPC Tiers</h3>

	<ul>
		<li>Tier-1 Facilities: ARCHER and DiRAC</li>
		<li>Tier-2 Facilities: (between Tier-1 and Tier-3!)</li>
		<li>Tier-3 Facilities: institutional facilities</li>
		<li>Other Facilities: e.g. public cloud</li>
	</ul>

	<p>Distinctions often blur between different tiers</p>
</section>

<section>
	<img class="plain" src="img/hpcuk_logo.png" alt="HPC-UK logo" />

	<p>Information on facilities and how to access them</p>
	<p><a href="http://www.hpc-uk.ac.uk">http://www.hpc-uk.ac.uk</a></p>

	<p>Open Source, community-developed resource</p>
</section>

<section>
	<h3>HPC Services Covered</h3>
	<ul>
		<li><a href="#/archer">ARCHER</a> - UK National Supercomputing Service</li>
		<li><a href="#/dirac">DiRAC</a> - STFC national HPC service</li>
		<li><a href="#/tier2">UK National Tier-2 HPC facilities</a></li>
		<li><a href="#/other">Other facilities</a></li>
	</ul>
</section>


<!-- ARCHER Section ------------------------------------------------------------------------ -->
<section id="archer">
	<img class="plain" src="img/ARCHER_logo.png" alt="ARCHER national supercomputing service logo" />
	<p><a href="http://www.archer.ac.uk">http://www.archer.ac.uk</a></p>
</section>

<section>
	<h4>UK National Supercomputing Service</h4>
	<div class="lblock">
		<img width="250" class="plain" src="img/epsrc-highres.jpg" alt="EPSRC logo" />
		<img width="150" class="plain" src="img/nerc-logo-large.jpg" alt="NERC logo" />
		<img width="250" class="plain" src="img/epcc_logo.png" alt="EPCC logo" />
		<img width="250" class="plain" src="img/cray-logo.png" alt="Cray Inc logo" />
	</div>
	<div class="rblock">
		<img class="plain" src="img/archer_cabs.png" alt="ARCHER cabinets at EPCC advanced computing facility" />
	</div>
</section>

<section>
	<h4>ARCHER Service Design</h4>
	<ul>
		<li>High core count distributed memory: 1000+ cores</li>
		<li>"Traditional" capability HPC</li>
		<li>Main users: Materials science, Climate/ocean modelling, CFD, <em>etc.</em></li>
		<li>Growing interest in non-traditional use: <em>e.g.</em> data science, bioinformatics</li>
	</ul>
	<p>Cray XC30: 118,080 cores (4920 nodes)<br/>2&times; 12-core Xeon per node; 64/128 GiB memory</p>
</section>

<section>
	<h4>ARCHER Support</h4>

	<p>Large amount of support provided by EPCC and Cray</p>

	<ul>
		<li>Helpdesk with access to EPCC and Cray expertise</li>
		<li>Free training across the UK</li>
		<li>eCSE calls for software development (final call now closed)</li>
		<li>Benchmarking/profiling work to support users</li>
	</ul>

	<p><strong>All free at point of use</strong></p>

</section>

<section>
	<h3>Access to ARCHER</h3>

	<dl style="font-size: 0.75em">
		<dt>Instant Access (EPSRC remit only)</dt>
		<dd>Small amounts of resource to test ARCHER facilities</dd>
		<dt>ARCHER Driving Test</dt>
		<dd>Complete simple test and gain small amounts of resource to test ARCHER facilities</dd>
		<dt>Resource Allocation Panel (RAP) (EPSRC remit only)</dt>
		<dd>Applications for large amounts of ARCHER resource for 1 year</dd>
		<dt>Grant Access</dt>
		<dd>Add access costs to grant application (notional for EPSRC/NERC, real otherwise)</dd>
	</dl>
	<p><a href="http://www.archer.ac.uk/access">http://www.archer.ac.uk/access</a></p>
</section>


<!-- DiRAC Section ------------------------------------------------------------------------ -->
<section id="dirac">
	<img class="plain" src="img/dirac.png" alt="DiRAC STFC national HPC service logo" />
	<p><a href="http://www.dirac.ac.uk">http://www.dirac.ac.uk</a></p>
</section>

<section>
	<h4>STFC National HPC Service</h4>
	<div class="lblock">
		<ul>
			<li>Range of different systems</li>
			<li>Covers capacity to capability use</li>
			<li>Main HPC resource for STFC</li>
		</ul>
		<img class="plain" src="img/stfc_logo.jpg" alt="STFC logo" />
	</div>
	<div class="rblock">
		<img class="plain" src="img/cosma4_cabs.jpg" alt="COSMA cabinets at the University of Durham" />
	</div>
</section>

<section>
	<p>A variety of HPC technology is used across DiRAC systems to meet different research challenges</p>
	<dl style="font-size: 0.75em">
		<dt>Extreme Scaling: University of Edinburgh</dt>
		<dd>Applications that scale to high core counts and require high interconnect performance</dd>
		<dt>Data Intensive: University of Cambridge, University of Leicester</dt>
		<dd>Analysis of large data sets using a combination of modelling and simulation</dd>
		<dt>Memory Intensive: University of Durham</dt>
		<dd>Detailed and complex simulations related to computational fluid dynamic problems</dd>
	</dl>
	<p><a href="https://www.dirac.ac.uk/Resources.html">https://www.dirac.ac.uk/Resources.html</a></p>
</section>

<section>
	<h3>Access to DiRAC</h3>

	<dl style="font-size: 0.75em">
		<dt>Seedcorn Access</dt>
		<dd>Small amounts of resource to test DiRAC facilities. Always available by
			contacting the DiRAC service.</dd>
		<dt>Resource Allocation Committee (RAC)</dt>
		<dd>Applications for large amounts of DiRAC resource for either 1 year (small
			projects) or 3 years (large projects).
	</dl>
	<p><a href="http://www.dirac.ac.uk/access.html">http://www.dirac.ac.uk/access.html</a></p>
</section>

<!-- Tier-2 Section ------------------------------------------------------------------------ -->
<section id="tier2">
	<h2>Tier2: National Facilities</h2>
	<img width="400" class="plain" src="img/tier2_logo.jpg" alt="Tier 2 national HPC logo" />
	<img width="300" class="plain" src="img/epsrc-highres.jpg" alt="EPSRC logo" />
</section>

<section>
	<p>Nationally accessible, located across the UK</p>
	<div class="lblock">
		<img class="plain" src="img/tier2_logos.png" alt="Logos of Tier 2 HPC hosting organisations" />
	</div>
	<div class="rblock">
		<img src="img/tier2-map.png" alt="Map of Tier 2 HPC hosting organisations" />
	</div>
</section>

<section>
	<h3>Intel Xeon Clusters</h3>
	<dl style="font-size: 0.75em">
		<dt>Cirrus@EPCC</dt>
			<dd>10,080 core Broadwell, FDR Infiniband Hypercube</dd>
		<dt>HPC Midlands+</dt>
			<dd>14,336 core Broadwell, 3:1 blocking EDR Infiniband (764 core non-blocking)</dd>
			<dd>10 POWER8 per nodes with 1 TB memory</dd>
		<dt>Cambridge CSD3</dt>
			<dd>24,576 core Skylake, Intel Omnipath</dd>
		<dt>Materials and Molecular Modelling Hub</dt>
			<dd>17,280 core Broadwell, 3:1 blocking Intel Omnipath (864 core non-blocking)</dd>
	</dl>
</section>

<section>
	<h3>Other architectures</h3>
	<dl style="font-size: 0.75em">
		<dt>JADE</dt>
			<dd>22 Nvidia DGX-1: 2&times; Xeon + 8&times; Nvidia P100 GPGPU</dd>
		<dt>Cambridge CSD3</dt>
			<dd>90 nodes: Xeon + 4&times; Nvidia P100 GPGPU, EDR Infiniband</dd>
			<dd>384 Xeon Phi nodes (96 GiB memory per node), Intel Omnipath</dd>
			<dd>50 node Hadoop Cluster</dd>
		<dt>Isambard@GW4</dt>
			<dd>10,000+ ARMv8 cores, Cray Aries Interconnect</dd>
			<dd>Due mid-2018</dd>
	</dl>
</section>


<section id="cirrus">
	<div class="lblock">
		<ul>
			<li><strong>Flexible HPC:</strong>
				<ul class="inner">
					<li>Traditional HPC</li>
					<li>Task farming</li>
					<li>Data science</li>

			 	</ul></li>
			<li><strong>Supported by EPCC experts:</strong>
				<ul class="inner">
					<li>Wide range of experience and expertise</li>
					<li>Free access to RSE expertise for all users - irrespective of location</li>
					<li>Close links with ARCHER technical support</li>
				</ul></li>
			<li><strong>Broad range of software preinstalled:</strong>
				<ul class="inner">
					<li>Traditional HPC (e.g. VASP, OpenFOAM, GROMACS)</li>
					<li>Data science (e.g. R, Spark, Hadoop)</li>
					<li>Containerisation and mobile compute (Singularity)</li>
		</ul>
	</div>
	<div class="rblock">
		<img class="plain" src="img/cirrus_PoweredbyEPCC.png" alt="Cirrus@EPCC logo" />
		<img class="plain" src="img/cirrus_cabs.png" alt="Cirrus cabinets at EPCC Advanced Computing Facility" />
		<p><a href="http://www.cirrus.ac.uk">http://www.cirrus.ac.uk</a></p>
	</div>
</section>

<section>
	<div class="lblock">
		<ul>
			<li><strong>Mixed workflows:</strong>
				<ul class="inner">
					<li>Colocate big comute and big data</li>
					<li>Facilitate complex workflows</li>
			 	</ul></li>
			<li><strong>Wide range of hardware:</strong>
				<ul class="inner">
					<li>Intel Xeon CPU cluster</li>
					<li>Intel Xeon Phi cluster</li>
					<li>NVidia GPU cluster</li>
					<li>Hadoop cluster</li>
				</ul></li>
			<li><strong>RSE support:</strong>
				<ul class="inner">
					<li>Collaborative/cooperative support model</li>
					<li>Based in Cambridge with additional support in some partner institutions</li>
				</ul></li>
		</ul>
	</div>
	<div class="rblock">
		<img width="200" class="plain" src="img/csd3_logo.png" alt="CSD3 logo" />
		<img class="plain" src="img/csd3_cabs.png" alt="CSD3 cabinets at the University of Cambridge" />
		<p><a href="https://www.csd3.cam.ac.uk/">https://www.csd3.cam.ac.uk/</a></p>
	</div>

</section>

<section>
	<div class="lblock">
		<ul>
			<li><strong>HPC Cluster:</strong>
				<ul class="inner">
					<li>Standard HPC jobs</li>
					<li>Wide range of software and tools</li>
			 	</ul></li>
			<li><strong>OpenPower System:</strong>
				<ul class="inner">
					<li>POWER8 + GPU + SSD file store</li>
					<li>Data analysis of large datasets</li>
					<li>Test bed for codes that are memory bandwidth limited</li>
					<li>On-the-fly data processing</li>
			 	</ul></li>
			<li><strong>RSE support:</strong>
				<ul class="inner">
					<li>Support for partner institutions</li>
					<li>Distributed across consortium</li>
				</ul></li>
		</ul>
	</div>
	<div class="rblock">
		<img class="plain" src="img/hpcmidplus_logo.png" alt="HPC Midlands + logo" />
		<img class="plain" src="img/hpcmidplus_cabs.jpg" alt="Athena cabinets at Lougborough University" />
		<p><a href="https://www.csd3.cam.ac.uk/">http://www.hpc-midlands-plus.ac.uk</a></p>
	</div>

</section>

<section>
	<div class="lblock">
		<ul>
			<li><strong>Exploring ARM technology:</strong>
				<ul class="inner">
					<li>Cray XC50 ARMv8 system (due mid 2018)</li>
					<li>Production HPC based on ARM</li>
					<li>Performance comparisons and benchmarking</li>
					<li>Cray software tools (compilers, libraries, etc.)</li>
			 	</ul></li>
			<li><strong>Support and expertise:</strong>
				<ul class="inner">
					<li>RSE effort provided by GW4 institutions</li>
					<li>Cray/ARM Centre of Excellence</li>
					<li>Training and Hackathons</li>
			 	</ul></li>
		</ul>
	</div>
	<div class="rblock">
		<img class="plain" width="175" src="img/GW4-logo_CMYK.jpg" alt="GW4 logo" />
		<img class="plain" width="200" src="img/isambard_logo.png" alt="Isambard logo" />
		<img class="plain" height="400" src="img/isambard_cabs.jpg" alt="Isambard cabinets at the UK Met Office" />
		<p><a href="http://gw4.ac.uk/isambard">http://gw4.ac.uk/isambard</a></p>
	</div>

</section>

<section>
	<div class="lblock">
		<ul>
			<li><strong>GPUs for research:</strong>
				<ul class="inner">
					<li>50% machine learning</li>
					<li>30% molecular dynamics</li>
					<li>20% other use</li>
					<li>Powerful individual nodes (8 GPUs)</li>
			 	</ul></li>
			<li><strong>Support and expertise:</strong>
				<ul class="inner">
					<li>Support for partner institutions</li>
					<li>Training committed by some partners</li>
			 	</ul></li>
		</ul>
	</div>
	<div class="rblock">
		<img class="plain" src="img/jade_logo.png" alt="JADE logo" />
		<img class="plain" src="img/Nvidia-DGX.jpg" alt="Image of NVidia DGX1 compute node" />
		<p><a href="http://www.jade.ac.uk">http://www.jade.ac.uk</a></p>
	</div>


</section>

<section>
	<div class="lblock">
		<ul>
			<li><strong>Support for Materials and Molecular Modelling:</strong>
				<ul class="inner">
					<li>Bridge gap between local resources and ARCHER</li>
					<li>Service specialised for MMM communities</li>
					<li>Community building within MMM domain</li>
			 	</ul></li>
			<li><strong>Access:</strong>
				<ul class="inner">
					<li>75% for partner institutions</li>
					<li>25% for non-partner via UKCP and MCC</li>
			 	</ul></li>
			<li><strong>Support and expertise:</strong>
				<ul class="inner">
					<li>Support for system provided by UCL research computing</li>
					<li>Named points of contact in partner institutions</li>
			 	</ul></li>
		</ul>
	</div>
	<div class="rblock">
		<img class="plain" src="img/mmmhubtransred.png" alt="Materials and Molecular Modelling Hub logo" />
		<img class="plain" src="img/mmmhub_cabs.jpg" alt="Thomas nodes at UCL" />
		<p><a href="https://mmmhub.ac.uk/">https://mmmhub.ac.uk/</a></p>
	</div>

</section>

<section>
	<h3>Access to Tier2</h3>

	<dl style="font-size: 0.75em">
		<dt>Instant and Pump-Priming Access</dt>
		<dd>Small amounts of resource to test Tier2 facilities</dd>
		<dd>Available on Cirrus and CSD3</dd>
		<dt>Resource Allocation Panel (RAP) (EPSRC remit only)</dt>
		<dd>Applications for large amounts of Tier2 resource for 1 year</dd>
		<dt>Grant Access</dt>
		<dd>Add access costs to grant application (notional for EPSRC, real otherwise)</dd>
		<dt>Materials and Molecular Modelling Hub</dt>
		<dd>Access via Materials Chemistry (MCC) or UK Car-Parrinello (UKCP) consortia</dd>
	</dl>

	<p>Other access routes may available for partners and local institutions. Check individual websites.</p>
</section>

<section>
	<dl style="font-size: 0.75em">
		<dt>Cirrus@EPCC</dt>
			<dd><a href="http://www.cirrus.ac.uk">http://www.cirrus.ac.uk</a></dd>
		<dt>HPC Midlands+</dt>
			<dd><a href="http://www.hpc-midlands-plus.ac.uk">http://www.hpc-midlands-plus.ac.uk</a></dd>
		<dt>Cambridge CSD3</dt>
			<dd><a href="http://www.csd3.cam.ac.uk">http://www.csd3.cam.ac.uk</a></dd>
		<dt>JADE</dt>
			<dd><a href="http://www.jade.ac.uk">http://www.jade.ac.uk</a></dd>
		<dt>Isambard@GW4</dt>
			<dd><a href=" http://gw4.ac.uk/isambard"> http://gw4.ac.uk/isambard</a></dd>
		<dt>Materials and Molecular Modelling Hub</dt>
			<dd><a href="https://mmmhub.ac.uk">https://mmmhub.ac.uk</a></dd>
	</dl>

</section>

<!-- Other Facilities -------------------------------------------------------------- -->

<section id="other">
	<h2>Other HPC Facilities</h2>
</section>

<section>
	<h3>International Facilities</h3>

	<dl style="font-size: 0.75em">
		<dt>PRACE</dt>
		<dd>Large-scale European facilities</dd>
		<dd>Number of different call types available</dd>
		<dd><a href="http://www.prace-ri.eu">http://www.prace-ri.eu</a></dd>
		<dt>US DoE INCITE</dt>
		<dd>Access to the largest HPC facilities in the USA</dd>
		<dd>Need to demonstrate need for extreme parallel scaling</dd>
		<dd><a href="http://www.doeleadershipcomputing.org/incite-program/">http://www.doeleadershipcomputing.org/incite-program/</a></dd>
	</dl>
</section>

<!-- Support Section ------------------------------------------------------------------------ -->

<section id="support">
	<h2>Getting Advice</h2>
	<img class="plain" src="img/mortar-launch-fail.gif" alt="Mortar launch gone wrong" />
</section>

<section>
	<h3>Helpdesks</h3>

	<p>All the sevices described provide a helpdesk service to answer queries on
	their services</p>

	<img class="plain" src="img/helpdesk.gif" alt="IT Crowd helpdesk with fire" />


</section>

<section>
	<h3>From local ARCHER/HPC Champions</h3>
	<p>Expertise in helping researchers choose the correct resource and use them effectively.</p>
	<p><a href="http://www.archer.ac.uk/community/champions/names/">http://www.archer.ac.uk/community/champions/names/</a></p>
	<img width="500" class="plain" src="img/ARCHER_logo.png" alt="ARCHER logo" />
</section>

<section>
	<img class="plain" src="img/rse_banner.png" alt="Research Software Engineers banner" />

	<p>Campaigning for the recognition of the RSE role, creating a community of RSE's and 
		organising events for RSE's to meet, exchange knowledge and collaborate.</p>

	<p>Join the community!</p>

	<p><a href="http://www.rse.ac.uk">http://www.rse.ac.uk</a></p>

</section>

<!-- Summary Section ------------------------------------------------------------------------ -->

<section id="summary">
	<h2>Summary</h2>
</section>

<section>
	<h3>Test Access</h3>

	<p><a href="http://www.archer.ac.uk">ARCHER Driving Test:</a> Access to ARCHER for any researcher based at UK institution</p>
	<p><a href="http://www.hpc-uk.ac.uk/access">Instant Access:</a> Access to ARCHER, Cirrus, CSD3</p>
	<p><a href="http://www.dirac.ac.uk/access.html">DiRAC Seedcorn:</a> Access to DiRAC</p>
</section>

<section>
	<h3>Specific Calls for Compute Time</h3>

	<p><a href="http://www.hpc-uk.ac.uk/access">EPSRC RAP:</a> Access to ARCHER and Tier2 for EPSRC-remit research. <strong>Open Now!</strong></p>
	<p><a href="http://www.dirac.ac.uk/access.html">DiRAC RAC:</a> Access to DiRAC for STFC-remit research</p>
</section>

<section>
	<h3>Access via Grant Proposals</h3>

	<p><a href="http://www.archer.ac.uk/access">ARCHER:</a> any funding body possible - notional costs for EPSRC/NERC</p>
	<p><a href="http://www.hpc-uk.ac.uk/access">Tier2:</a> any funding body possible - notional costs for EPSRC</p>
	<p><a href="http://www.dirac.ac.uk/access.html">DiRAC:</a> no time via grant applications - DiRAC RAC used instead</p>
</section>

<section>
	<ul>
		<li>Wide range of HPC facilities and technologies available to UK researchers</li>
		<li>Different access mechanisms for different research areas
			<ul class="inner">
				<li>RAP calls for access to ARCHER and Tier2 now open - close 10 Apr 2018</li>
			</ul></li>
		<li>Lots of ways to get support if you need it</li>
	</ul>
	<p><a href="http://www.hpc-uk.ac.uk">http://www.hpc-uk.ac.uk</a></p>

</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: false,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
